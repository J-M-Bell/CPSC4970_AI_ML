{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934c0083",
   "metadata": {},
   "source": [
    "<h1><strong><u>RNN Text Model</u></strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e9a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from keras import Input, activations\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import SimpleRNN, Dense, LSTM, Dropout, Embedding\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831ea7e",
   "metadata": {},
   "source": [
    "<h2><strong><u>Data Preprocessing Methods</u></strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14326f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    new_text = text.lower() #lowercase\n",
    "    new_text = re.sub(r\"([^\\w\\s])\", \"\", new_text) #remove punctuation\n",
    "    text_array = word_tokenize(new_text) #tokenize\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51571cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Given a list of words, I encode it word by word with each word being a sample). \n",
    "    I return the result and the encoder.\"\"\"\n",
    "    new_text = text.lower() #lowercase\n",
    "    new_text = re.sub(r\"([^\\w\\s])\", \"\", new_text) #remove punctuation\n",
    "    text_array = word_tokenize(new_text) #tokenize\n",
    "    info(\"Encoding inputs...\")\n",
    "    debug(f\"{text_array}\")\n",
    "    encoder = OrdinalEncoder()\n",
    "    #result = encoder.fit_transform(text)\n",
    "    result = encoder.fit_transform(np.reshape(text_array, (len(text_array), 1)))\n",
    "    # info(\"Number of input characters:\", len(encoder.categories_[0]))\n",
    "    # debug(\"Input categories:\", encoder.categories_[0])\n",
    "    # info(f\"{result.shape=}\")\n",
    "    # debug(result)\n",
    "    #print(result.shape)\n",
    "    return result, encoder\n",
    "\n",
    "\n",
    "# encoded_array, encoder = encode_text(text)\n",
    "# features, targets = time_delayed(encoded_array, 5)\n",
    "# print(\"Features:\")\n",
    "# print(features.shape)\n",
    "# print(\"Targets:\")\n",
    "# print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1342e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_value = max(encoded_array)\n",
    "# print(max_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14cd62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(encoded_array.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ff6e1",
   "metadata": {},
   "source": [
    "<h2><u>RNN Class</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af66067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_DEBUG = False\n",
    "PRINT_INFO = True\n",
    "\n",
    "\n",
    "def debug(*args):\n",
    "    if PRINT_DEBUG:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def info(*args):\n",
    "    if PRINT_INFO:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20303049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delayed(seq, delay):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for target_index in range(delay, len(seq)):\n",
    "        features.append(seq[target_index - delay:target_index])\n",
    "        targets.append(seq[target_index])\n",
    "    return np.array(features), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abecf474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this will need to be modified to handle words instead of letters\n",
    "# def encode_sequence(sequence):\n",
    "#     \"\"\"Given a string, I encode it letter by letter (each letter is a sample). I return the\n",
    "#     result and the encoder.\"\"\"\n",
    "#     info(\"Encoding inputs...\")\n",
    "#     debug(f\"{sequence}\")\n",
    "#     encoder = OrdinalEncoder(sparse=False)\n",
    "#     result = encoder.fit_transform(np.reshape(sequence, (len(sequence), 1)))\n",
    "#     info(\"Number of input characters:\", len(encoder.categories_[0]))\n",
    "#     debug(\"Input categories:\", encoder.categories_[0])\n",
    "#     info(f\"{result.shape=}\")\n",
    "#     debug(result)\n",
    "#     return result, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225c14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextModel:\n",
    "    def __init__(self, training_string, delay_length=100):\n",
    "        encoded_training_data, self.encoder = encode_text(training_string)\n",
    "        self.time_steps = delay_length\n",
    "        max_vocabulary_size = len(set(encoded_training_data.flatten()))\n",
    "        info(\"Number of distinct words:\", max_vocabulary_size)\n",
    "        debug(\"encoded_training_data:\", encoded_training_data)\n",
    "        self.X_delayed, self.y_delayed = time_delayed(encoded_training_data, self.time_steps)\n",
    "        self.model = self.create_model(max_vocabulary_size, self.X_delayed.shape, self.y_delayed.shape)\n",
    "\n",
    "    #figure out shape issues to see which values to use for embedding layer\n",
    "    def create_model(max_vocabulary_size, input_shape, output_shape, delay_length=100):\n",
    "        info(\"Creating model...\")\n",
    "        info(\"Input shape:\", input_shape[1:])\n",
    "        model = Sequential(\n",
    "            [Input(shape=input_shape[1:]),\n",
    "            Embedding(input_dim=max_vocabulary_size, output_dim=64, input_length=delay_length),\n",
    "            LSTM(256, return_sequences=True, activation=activations.tanh),\n",
    "            Dropout(0.2),\n",
    "            LSTM(256, activation=activations.tanh),\n",
    "            Dropout(0.2),\n",
    "            Dense(output_shape[1], activation=activations.softmax)]\n",
    "        )\n",
    "        model.summary()\n",
    "        model.compile(optimizer=\"adam\", loss=CategoricalCrossentropy(), metrics=[\"categorical_accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    def encode_input_string(self, string):\n",
    "        encoded_input_array = encode_text(string)\n",
    "        return encoded_input_array\n",
    "    \n",
    "    def fit(self, prefix, epochs=2):\n",
    "        info(\"Fitting...\")\n",
    "        callbacks = []\n",
    "        if prefix is not None:\n",
    "            checkpoint = ModelCheckpoint(prefix + \"-{epoch:03d}-{loss:.4f}.hdf5\", monitor='loss', verbose=1,\n",
    "                                         save_best_only=True, mode='min')\n",
    "            callbacks = [checkpoint]\n",
    "        self.model.fit(self.X_delayed, self.y_delayed, epochs=epochs, verbose=True, callbacks=callbacks, batch_size=1000)\n",
    "\n",
    "    def load_weights(self, filename):\n",
    "        info(f\"Loading weights from {filename}...\")\n",
    "        self.model.load_weights(filename)\n",
    "    \n",
    "    def predict_from_seed(self, seed, prediction_count):\n",
    "        info(\"Predicting output sequence...\")\n",
    "        result = seed\n",
    "        new_seed = seed\n",
    "        for i in range(prediction_count):\n",
    "            inp = self.encode_input_string(new_seed)\n",
    "            debug(f\"{inp=}\")\n",
    "            p = self.encoder.inverse_transform(self.model.predict(inp))\n",
    "            debug(f\"{p=}\")\n",
    "            print(p.shape)\n",
    "            print(f\"Predicted word: {p[0][0]}\")\n",
    "            result += p[0][0]\n",
    "            new_seed = result[-len(seed):]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(text, seed):\n",
    "    rnn_model = RNNTextModel(text, delay_length=5)\n",
    "    rnn_model.fit(prefix=\"rnn_text_model\", epochs=10)\n",
    "    output = rnn_model.predict_from_seed(seed, 10)\n",
    "    print(f\"Generated text: {seed} {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9deb6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"./the_sunken_world.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'your_file.txt' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
